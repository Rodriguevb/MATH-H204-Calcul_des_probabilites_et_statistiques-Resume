\newpage
\subsection{Théorèmes fondamentaux}






\subsubsection{Inégalité de Bienaymé-Tchebycheff}
La proportion d'individus s'écartant de la moyenne d'une distribution de plus $k$ fois l'écart-type ($\sigma$) ne dépasse jamais $\dfrac{1}{k^2}$ :
$$\boxed{\dfrac{1}{k^2} \geq P(|V-\mu|\geq k\sigma)}$$
\paragraph{Démonstration}
\subparagraph{Cas continu}
\begin{center}
	$\begin{array}{LL}
		\sigma_2       &= \int_{-\infty}^{\infty} (x-\mu)^2\ f(x)\ dx\\
			     &= \underbrace{\int_{-\infty}^{\mu-k\sigma}}_{\substack{x\leq\mu-k\sigma\\x-\mu\leq -k\sigma\\(x-\mu)^2\geq(k\sigma)^2}}
		         (x-\mu)^2\ f(x)\ dx + \underbrace{\int_{\mu-k\sigma}^{\mu+k\sigma}}_{\geq0} (x-\mu)^2\ f(x)\ dx + \underbrace{\int_{\mu+k\sigma}^{\infty}}_{\substack{x\geq\mu+k\sigma\\x-\mu\geq k\sigma\\(x-\mu)^2\geq(k\sigma)^2}} (x-\mu)^2\ f(x)\ dx\\
		\sigma^2       &\geq \int_{-\infty}^{\mu-k\sigma} (k\sigma)^2\ f(x)\ dx + \int_{\mu+k\sigma}^{\infty} (k\sigma)^2\ f(x)\ dx\\
			     &\geq (k\sigma)^2\ \int_{-\infty}^{\mu-k\sigma} f(x)\ dx + (k\sigma)^2\ \int_{\mu+k\sigma}^{\infty} f(x)\ dx\\
		\dfrac{1}{k^2} &\geq P( V - \mu \leq-k\sigma) + P( V - \mu \geq k\sigma)\\
			     &\geq P(|V-\mu|\geq k\sigma) + P(|V-\mu|\geq k\sigma)\\
	\end{array}$
\end{center}
$$\boxed{\dfrac{1}{k^2} \geq P(|V-\mu|\geq k\sigma)}$$
\subparagraph{Cas discret}
\begin{align*}
	\sigma^2 &= \sum_{i} p_i(x_i - \mu)^2\\
	         &= \sum_{i;x_i\leq\mu-k\sigma} p(x_i - \mu)^2 + \sum_{i;\mu-k\sigma < x_i < \mu+k\sigma} p(x_i - \mu)^2 + \sum_{i;\mu+k\sigma\leq x_i} p(x_i - \mu)^2\\
	\sigma^2 &\geq\sum_{i;x_i\leq\mu-k\sigma} p(x_i - \mu)^2 + \sum_{i;\mu+k\sigma\leq x_i} p(x_i - \mu)^2\\
	         &\geq k^2\sigma^2 \sum_{i;x_i\leq\mu-k\sigma} p_i + \sum_{i;\mu+k\sigma\leq x_i} p_i\\
	\dfrac{1}{k^2} &\geq \sum_{i;x_i\leq\mu-k\sigma} p_i + \sum_{i;\mu+k\sigma\leq x_i} p_i\\
	         &\geq P(V \leq \mu - k\sigma) + P(V \geq \mu +k\sigma)\\
	         &\geq P(V - \mu \leq - k\sigma) + P(V - \mu \geq k\sigma)\\
	         &\geq P(|V - \mu| \geq k\sigma) + P(|V - \mu| \geq k\sigma)\\
\end{align*}
$$\boxed{\dfrac{1}{k^2} \geq P(|V - \mu| \geq k\sigma)}$$








\newpage
\subsubsection{Théorème de Bernouilli ou loi des grands nombres}
Lors de $n$ répétitions d'une expérience aléatoire, la fréquence relative $\dfrac{F}{n}$ d'un évènement tend vers sa probabilité $p$ d'exister lorsque $n\rightarrow\infty$
$$\boxed{\dfrac{p(1-p)}{n\epsilon^2} \geq P\left(\left|\dfrac{F}{n}-p\right|\geq\epsilon\right) \stackrel{n\rightarrow\infty}{\rightarrow} 0}$$
\paragraph{Démonstration}
On part avec le théorème de Bienaymé-Tchebycheff
$$\dfrac{1}{k^2} \geq P(|V-\mu|\geq k\sigma)$$
Et on considère une binomiale $V = B(n,p)$
\begin{align*}
	\dfrac{1}{k^2} &\geq P\left(\left|B(n,p)-np\right|\geq k\sqrt{np(1-p)}\right)\\
                   &\geq P\left(\dfrac{\left|B(n,p)-np\right|}{n}\geq \dfrac{k\sqrt{np(1-p)}}{n}\right)\\
                   &\geq P\left(\left|\dfrac{B(n,p)}{n}-p\right|\geq k\sqrt{\dfrac{np(1-p)}{n^2}}\right)\\
                   &\geq P\left(\left|\dfrac{B(n,p)}{n}-p\right|\geq k\sqrt{\dfrac{p(1-p)}{n}}\right)
\end{align*}
On pose $k\sqrt{\dfrac{p(1-p)}{n}} = \epsilon$ et $B(n,p) = F$
\begin{center}
	$\left\{\begin{array}{LLL}
		k\sqrt{\dfrac{p(1-p)}{n}}&=&\epsilon\\
		k&=&\epsilon\dfrac{\sqrt{n}}{\sqrt{p(1-p)}}\\
		k^2&=&\dfrac{\epsilon^2n}{p(1-p)}\\
		\dfrac{1}{k^2}&=&\dfrac{p(1-p)}{\epsilon^2n}\\
	\end{array}\right.$
\end{center}
$$\boxed{\dfrac{p(1-p)}{n\epsilon^2} \geq P\left(\left|\dfrac{F}{n}-p\right|\geq\epsilon\right) \stackrel{n\rightarrow\infty}{\rightarrow} 0}$$







\newpage
\subsubsection{Théorème Central-Limite}
Ce théorème stipule que si $V$ est une somme de $n$ variables aléatoires (quelconques) indépendantes
$$V = X_1\ ,\ X_2\ ,\ X_3\ ,\ \dots\ ,\ X_n$$
alors sa variable réduite $\dfrac{V-\mu}{\sigma}$ tend vers une gausienne $N(0,1)$ lorsque $n\rightarrow\infty$.
$$\boxed{\dfrac{V-E(V)}{D(V)} = \dfrac{V-\mu}{\sigma} \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)}$$
\paragraph{Démonstration}
On pose d'abord les calculs de l'espérance et de l'écart-type de la somme de variable aléatoire $V$, pour un de ses éléments $V_i$, pour la somme de variable centrée $W$ et pour un de ses éléments $W_i$:
\begin{center}
$\left\{\begin{array}{LCLC}
V        &=& \sum_{i=1}^n V_i&(1)\\
E( V_i ) &=& \widehat{\mu}&(2)\\
D( V_i ) &=& \widehat{\sigma}&(3)\\
E( V )   &=& \sum_{i=1}^n E (V_i)\\
         &=& n\widehat{\mu}&(4)\\
D( V )   &=& \sqrt{D^2(V)}\\
         &=& \sqrt{\sum_{i=1}^n D^2(V_i)}\\
         &=& \sqrt{n\widehat{\sigma}^2}\\
         &=& \sqrt{n}\ \widehat{\sigma}&(5)\\
\end{array}\right.
\left\{\begin{array}{LCLC}
W_i      &=& V_i - \widehat{\mu}&(6)\\
W        &=& \sum_{i=1}^n W_i\\
         &=& \sum_{i=1}^n  (V_i - \widehat{\mu})\\
         &=& \sum_{i=1}^n  (V_i) - n\widehat{\mu}\\
         &=& V - E ( V )&(7)\\
E(W_i)   &=& E ( V_i - \widehat{\mu} )\\
         &=& E( V_i ) - \widehat{\mu}\\
         &=& \widehat{\mu} - \widehat{\mu} = 0&(8)\\
D^2(W_i) &=& D^2( V_i - \widehat{\mu} )\\
         &=& D^2( V_i )\\
         &=& \widehat{\sigma}^2&(9)\\
\end{array}\right.$
\end{center}

On commence avec la fonction génératrice des moments $\psi_V(t) = E(e^{tV})$
\begin{align*}
	\psi_Z(t) &= E \left( e^{tZ} \right)\\
              &= E \left( e^{(t\dfrac{V-\mu}{\sigma})} \right)\\
              &= E \left( \displaystyle e^{(\dfrac{t}{\sigma}(V-\mu))} \right)\\
\intertext{On remplace par $\left\{
\begin{array}{LCLC}
V - \mu &=& \sum_{i=1}^n V_i - \mu&\text{car propriété }(1)\\
        &=& \sum_{i=1}^n V_i - E ( V )&\text{car propriété }(4)\\
        &=& \sum_{i=1}^n V_i - E \left( \sum_{i=1}^n V_i \right)&\text{car propriété }(1)\\
        &=& \sum_{i=1}^n V_i - \sum_{i=1}^n E \left( V_i \right)\\
        &=& \sum_{i=1}^n V_i - \sum_{i=1}^n \widehat{\mu}&\text{car propriété }(2)\\
        &=& \sum_{i=1}^n V_i - \widehat{\mu}
\end{array}
\right.$}
              &= E \left( \displaystyle e^{(\dfrac{t}{\sigma} \displaystyle\sum_{i=1}^n (V_i - \widehat{\mu} ))} \right)\\
              &= E \left( \displaystyle e^{(\dfrac{t}{\sigma} \displaystyle\sum_{i=1}^n ( W_i ))} \right)&\text{car propriété }(6)\\
\intertext{Propriété : $Z=V+W$(indépendant)$\left\{
\begin{aligned}
\psi_Z &= E ( e^{tZ} )\\
       &= E ( e^{t(V+W)} )\\
       &= E ( e^{tV}e^{tW} )\\
       &= E ( e^{tV} ) E ( e^{tW} )\\
       &= \psi_V \psi_W
\end{aligned}
\right.$}
             &= \psi_{W_1}\left(\dfrac{t}{\sigma}\right) \psi_{W_2}\left(\dfrac{t}{\sigma}\right)\ \dots\ \psi_{W_n}\left(\dfrac{t}{\sigma}\right)\\
             &= \left( \psi_{W_i}\left(\dfrac{t}{\sigma}\right) \right)^n
\end{align*}
$$\boxed{\psi_Z(t) = E \left( e^{tZ} \right) = \left( \psi_{W_i}\left(\dfrac{t}{\sigma}\right) \right)^n}$$
Maintenant nous développons $\psi_{W_i}(t)$ par le théorème de Taylor :
\begin{center}
$\left\{\begin{array}{LCL}
f(x) &=& \displaystyle\sum_{k=0}^n \dfrac{f^{(k)}(a)}{k!}(x-a)^k + R_n(x)\\
     &=& f(a) + \dfrac{f'(a)}{1!}(x - a) + \dfrac{f^{(2)}(a)}{2!}(x - a)^2 + \cdots + \dfrac{f^{(n)}(a)}{n!}(x - a)^n + R_n(x)\\
\end{array}\right.$
\end{center}
\begin{center}
$\boxed{\begin{array}{LCL}
D^2(W_i) &=& E(W_i^2) - E^2(W_i)\\
E(W_i^2) &=& D^2(W_i) + E^2(W_i)\\
         &=& \widehat{\sigma}^2 + 0^2\\
\end{array}}$
\end{center}
\begin{center}
$\left\{\begin{array}{LCLC}
\psi_{W_i}(t) &=& 1 + E ( W_i ) t + E (W_i^2) \dfrac{t^2}{2}+R(t^3)\\
              &=& 1 + 0 t + \widehat{\sigma}^2 \dfrac{t^2}{2} + R(t^3)&\text{car propriété (8) et (9)}\\
\end{array}\right.$
\end{center}
$$\boxed{Z = \dfrac{W}{\sqrt{n}\widehat{\sigma}} = \dfrac{V-\mu}{\sigma} \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)}$$
\begin{center}
$\begin{array}{LCL}
\lim_{n\rightarrow\infty}\ \psi_{Z}(t) &=& \lim_{n\rightarrow\infty}\ \left( \psi_{W_i} \left( \dfrac{t}{\sqrt{n}\widehat{\sigma}} \right)^n \right) \\[0.3cm]
            &=& \lim_{n\rightarrow\infty}\ \left( 1 + \dfrac{t^2}{2n} + R(t^3) \right)^n\\
            &=& \lim_{n\rightarrow\infty}\ \left( 1 + \dfrac{t^2}{2n} \right)^n\\
            &=& e^{\frac{t^2}{2}}
\end{array}$
\end{center}
On a donc une allure d'une fonction génératrice d'une $N(0,1)$
$$\boxed{\psi_{Z}(t) \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)}$$









\newpage
\subsubsection{Théorème de De Moivre}
C'est un cas particulier du théorème Central-Limite puisqu'une binomiale est bien une somme de variables quelconques de mêmes distributions (à savoir, des variables indicatrices). La variable binomiale est asymptotiquement normale lorsque $\rightarrow\infty$.
$$\boxed{\dfrac{B(n,p)-np}{\sqrt{np(1-p)}} \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)}$$
\paragraph{Démonstration}
On part donc avec le théorème de Central-Limite :
$$\dfrac{V-E(V)}{D(V)} = \dfrac{V-\mu}{\sigma} \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)$$
Et on considère une binomiale $V = B(n,p)$.
$$\boxed{\dfrac{B(n,p)-np}{\sqrt{np(1-p)}} \stackrel{n\rightarrow\infty}{\rightarrow} N(0,1)}$$

